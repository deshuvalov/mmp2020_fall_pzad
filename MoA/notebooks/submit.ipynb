{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-22T16:29:52.373945Z",
     "iopub.status.busy": "2020-10-22T16:29:52.373166Z",
     "iopub.status.idle": "2020-10-22T16:29:54.470326Z",
     "shell.execute_reply": "2020-10-22T16:29:54.469159Z"
    },
    "papermill": {
     "duration": 2.114249,
     "end_time": "2020-10-22T16:29:54.470452",
     "exception": false,
     "start_time": "2020-10-22T16:29:52.356203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#used net arch from kaggle.com/nicohrubec/pytorch-multilabel-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.special import erf, erfinv\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import FLOAT_DTYPES, check_array, check_is_fitted\n",
    "\n",
    "\n",
    "class GaussRankScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transform features by scaling each feature to a normal distribution.\n",
    "    Parameters\n",
    "        ----------\n",
    "        epsilon : float, optional, default 1e-4\n",
    "            A small amount added to the lower bound or subtracted\n",
    "            from the upper bound. This value prevents infinite number\n",
    "            from occurring when applying the inverse error function.\n",
    "        copy : boolean, optional, default True\n",
    "            If False, try to avoid a copy and do inplace scaling instead.\n",
    "            This is not guaranteed to always work inplace; e.g. if the data is\n",
    "            not a NumPy array, a copy may still be returned.\n",
    "        n_jobs : int or None, optional, default None\n",
    "            Number of jobs to run in parallel.\n",
    "            ``None`` means 1 and ``-1`` means using all processors.\n",
    "        interp_kind : str or int, optional, default 'linear'\n",
    "           Specifies the kind of interpolation as a string\n",
    "            ('linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',\n",
    "            'previous', 'next', where 'zero', 'slinear', 'quadratic' and 'cubic'\n",
    "            refer to a spline interpolation of zeroth, first, second or third\n",
    "            order; 'previous' and 'next' simply return the previous or next value\n",
    "            of the point) or as an integer specifying the order of the spline\n",
    "            interpolator to use.\n",
    "        interp_copy : bool, optional, default False\n",
    "            If True, the interpolation function makes internal copies of x and y.\n",
    "            If False, references to `x` and `y` are used.\n",
    "        Attributes\n",
    "        ----------\n",
    "        interp_func_ : list\n",
    "            The interpolation function for each feature in the training set.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-4, copy=True, n_jobs=None, interp_kind='linear', interp_copy=False):\n",
    "        self.epsilon = epsilon\n",
    "        self.copy = copy\n",
    "        self.interp_kind = interp_kind\n",
    "        self.interp_copy = interp_copy\n",
    "        self.fill_value = 'extrapolate'\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit interpolation function to link rank with original data for future scaling\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The data used to fit interpolation function for later scaling along the features axis.\n",
    "        y\n",
    "            Ignored\n",
    "        \"\"\"\n",
    "        X = check_array(X, copy=self.copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        self.interp_func_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit)(x) for x in X.T)\n",
    "        return self\n",
    "\n",
    "    def _fit(self, x):\n",
    "        x = self.drop_duplicates(x)\n",
    "        rank = np.argsort(np.argsort(x))\n",
    "        bound = 1.0 - self.epsilon\n",
    "        factor = np.max(rank) / 2.0 * bound\n",
    "        scaled_rank = np.clip(rank / factor - bound, -bound, bound)\n",
    "        return interp1d(\n",
    "            x, scaled_rank, kind=self.interp_kind, copy=self.interp_copy, fill_value=self.fill_value)\n",
    "\n",
    "    def transform(self, X, copy=None):\n",
    "        \"\"\"Scale the data with the Gauss Rank algorithm\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The data used to scale along the features axis.\n",
    "        copy : bool, optional (default: None)\n",
    "            Copy the input X or not.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'interp_func_')\n",
    "\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._transform)(i, x) for i, x in enumerate(X.T))).T\n",
    "        return X\n",
    "\n",
    "    def _transform(self, i, x):\n",
    "        return erfinv(self.interp_func_[i](x))\n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        \"\"\"Scale back the data to the original representation\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data used to scale along the features axis.\n",
    "        copy : bool, optional (default: None)\n",
    "            Copy the input X or not.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'interp_func_')\n",
    "\n",
    "        copy = copy if copy is not None else self.copy\n",
    "        X = check_array(X, copy=copy, estimator=self, dtype=FLOAT_DTYPES, force_all_finite=True)\n",
    "\n",
    "        X = np.array(Parallel(n_jobs=self.n_jobs)(delayed(self._inverse_transform)(i, x) for i, x in enumerate(X.T))).T\n",
    "        return X\n",
    "\n",
    "    def _inverse_transform(self, i, x):\n",
    "        inv_interp_func = interp1d(self.interp_func_[i].y, self.interp_func_[i].x, kind=self.interp_kind,\n",
    "                                   copy=self.interp_copy, fill_value=self.fill_value)\n",
    "        return inv_interp_func(erf(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_duplicates(x):\n",
    "        is_unique = np.zeros_like(x, dtype=bool)\n",
    "        is_unique[np.unique(x, return_index=True)[1]] = True\n",
    "        return x[is_unique]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T16:37:34.681948Z",
     "iopub.status.busy": "2020-10-22T16:37:34.681054Z",
     "iopub.status.idle": "2020-10-22T16:37:37.116522Z",
     "shell.execute_reply": "2020-10-22T16:37:37.115565Z"
    },
    "papermill": {
     "duration": 2.569225,
     "end_time": "2020-10-22T16:37:37.116651",
     "exception": false,
     "start_time": "2020-10-22T16:37:34.547426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "submit[targets] = preds\n",
    "submit.loc[X_test['cp_type']=='ctl_vehicle', targets] = 0\n",
    "submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CDLL 'caffe2_nvrtc.dll', handle 7ffae3e70000 at 0x21c83456f10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ctypes\n",
    "ctypes.cdll.LoadLibrary('caffe2_nvrtc.dll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_min = 1e-15\n",
    "p_max = 1 - p_min\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_pred = np.clip(y_pred, p_min, p_max)\n",
    "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv('../input/lish-moa/train_features.csv', index_col='sig_id')\n",
    "test_Y = pd.read_csv('../input/lish-moa/sample_submission.csv', index_col='sig_id')\n",
    "train_Y = pd.read_csv('../input/lish-moa/train_targets_scored.csv', index_col='sig_id', dtype={f: test_Y.dtypes[f] for f in test_Y})\n",
    "test_X = pd.read_csv('../input/lish-moa/test_features.csv', index_col='sig_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.cp_time = train_X.cp_time / 24\n",
    "test_X.cp_time = test_X.cp_time / 24\n",
    "\n",
    "train_X['real_drug'] = train_X.cp_type == 'trt_cp'\n",
    "test_X['real_drug'] = test_X.cp_type == 'trt_cp'\n",
    "\n",
    "t = train_X.cp_dose.copy()\n",
    "train_X.drop(columns=['cp_dose', 'cp_type'], inplace=True)\n",
    "train_X['cp_dose'] = 1\n",
    "train_X.loc[(t == 'D2'), 'cp_dose'] = 2\n",
    "\n",
    "t = test_X.cp_dose.copy()\n",
    "test_X.drop(columns=['cp_dose', 'cp_type'], inplace=True)\n",
    "test_X['cp_dose'] = 1\n",
    "test_X.loc[(t == 'D2'), 'cp_dose'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 6\n",
    "nstarts = 1\n",
    "nepochs = 50\n",
    "batch_size = 128\n",
    "val_batch_size = batch_size * 4\n",
    "criterion = nn.BCELoss()\n",
    "kfold = MultilabelStratifiedKFold(n_splits=nfolds, random_state=517, shuffle=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T16:30:02.716430Z",
     "iopub.status.busy": "2020-10-22T16:30:02.715242Z",
     "iopub.status.idle": "2020-10-22T16:30:02.717724Z",
     "shell.execute_reply": "2020-10-22T16:30:02.718219Z"
    },
    "papermill": {
     "duration": 0.019882,
     "end_time": "2020-10-22T16:30:02.718337",
     "exception": false,
     "start_time": "2020-10-22T16:30:02.698455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset_my(Dataset):\n",
    "    def __init__(self, df, targets, mode='train'):\n",
    "        self.mode = mode\n",
    "        #self.feats = feats_idx\n",
    "        #self.data = df[:, feats_idx]\n",
    "        self.data = df\n",
    "        if mode=='train':\n",
    "            self.targets = targets\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            return torch.FloatTensor(self.data[idx]), torch.FloatTensor(self.targets[idx])\n",
    "        elif self.mode == 'test':\n",
    "            return torch.FloatTensor(self.data[idx]), 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T16:30:02.755896Z",
     "iopub.status.busy": "2020-10-22T16:30:02.754888Z",
     "iopub.status.idle": "2020-10-22T16:37:28.495451Z",
     "shell.execute_reply": "2020-10-22T16:37:28.496258Z"
    },
    "papermill": {
     "duration": 445.769273,
     "end_time": "2020-10-22T16:37:28.496500",
     "exception": false,
     "start_time": "2020-10-22T16:30:02.727227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_CV_for_model(cur_model, model_num, train_X_loc, train_Y_loc, test_X_loc):\n",
    "    set_seed(seed)\n",
    "    for n, (tr, te) in enumerate(kfold.split(train_Y_loc, train_Y_loc)):\n",
    "        print(f'Train fold {n+1}')\n",
    "        xtrain, xval = train_X_loc[tr], train_X_loc[te]\n",
    "        ytrain, yval = train_Y_loc[tr], train_Y_loc[te]\n",
    "\n",
    "        train_set = Dataset_my(xtrain, ytrain)\n",
    "        val_set = Dataset_my(xval, yval)\n",
    "\n",
    "        dataloaders = {\n",
    "            'train': DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
    "            'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False)\n",
    "        }\n",
    "\n",
    "        model = cur_model(train_X_loc.shape[1]).to(device)\n",
    "        Path(f'./saved_params/model{model_num}').mkdir(parents=True, exist_ok=True)\n",
    "        checkpoint_path = f'./saved_params/model{model_num}/repeat_{1}_Fold_{n+1}.pt'\n",
    "        optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, eps=1e-4, verbose=True)\n",
    "        best_loss = {'train': np.inf, 'val': np.inf}\n",
    "\n",
    "        for epoch in range(nepochs):\n",
    "            epoch_loss = {'train': 0.0, 'val': 0.0}\n",
    "\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "                for i, (x, y) in enumerate(dataloaders[phase]):\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase=='train'):\n",
    "                        preds = model(x)\n",
    "                        loss = criterion(preds, y)\n",
    "\n",
    "                        if phase=='train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() / len(dataloaders[phase])\n",
    "\n",
    "                epoch_loss[phase] = running_loss\n",
    "\n",
    "            print(\"Epoch {}/{}   -   loss: {:5.5f}   -   val_loss: {:5.5f}\".format(epoch+1, nepochs, epoch_loss['train'], epoch_loss['val']))\n",
    "\n",
    "            scheduler.step(epoch_loss['val'])\n",
    "\n",
    "            if epoch_loss['val'] < best_loss['val']:\n",
    "                best_loss = epoch_loss\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T16:37:29.873707Z",
     "iopub.status.busy": "2020-10-22T16:37:29.872605Z",
     "iopub.status.idle": "2020-10-22T16:37:34.418673Z",
     "shell.execute_reply": "2020-10-22T16:37:34.419312Z"
    },
    "papermill": {
     "duration": 4.694514,
     "end_time": "2020-10-22T16:37:34.419497",
     "exception": false,
     "start_time": "2020-10-22T16:37:29.724983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_predict_for_model(cur_model, model_num, test_X_loc):\n",
    "    preds = np.zeros((test_X_loc.shape[0], test_Y.shape[1], nfolds))\n",
    "    \n",
    "    for n in range(nfolds):\n",
    "        test_set = Dataset_my(test_X_loc, None, mode='test')\n",
    "        dataloader = DataLoader(test_set, batch_size=val_batch_size, shuffle=False)\n",
    "        \n",
    "        checkpoint_path = f'./saved_params/model{model_num}/repeat_{1}_Fold_{n+1}.pt'\n",
    "        model = cur_model(test_X_loc.shape[1]).to(device)\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "        model.eval()\n",
    "        \n",
    "        fold_preds = []\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fold_preds.append(model(x))\n",
    "            \n",
    "        fold_preds = torch.cat(fold_preds, dim=0).cpu().numpy()\n",
    "        preds[:, :, n] = fold_preds\n",
    "    preds = preds.mean(axis=2)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T16:30:02.686535Z",
     "iopub.status.busy": "2020-10-22T16:30:02.684652Z",
     "iopub.status.idle": "2020-10-22T16:30:02.687231Z",
     "shell.execute_reply": "2020-10-22T16:30:02.687689Z"
    },
    "papermill": {
     "duration": 0.025318,
     "end_time": "2020-10-22T16:30:02.687824",
     "exception": false,
     "start_time": "2020-10-22T16:30:02.662506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model4(nn.Module):\n",
    "    def __init__(self, num_columns):\n",
    "        super(Model4, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 2048))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(2048, 1024))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(1024)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(1024, 206))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.sigmoid(self.dense3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T16:30:01.961409Z",
     "iopub.status.busy": "2020-10-22T16:30:01.960089Z",
     "iopub.status.idle": "2020-10-22T16:30:02.200795Z",
     "shell.execute_reply": "2020-10-22T16:30:02.199707Z"
    },
    "papermill": {
     "duration": 0.257815,
     "end_time": "2020-10-22T16:30:02.200918",
     "exception": false,
     "start_time": "2020-10-22T16:30:01.943103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = test_Y[test_X['real_drug'] == False]\n",
    "for f in t:\n",
    "    t[f] = 0\n",
    "train_Y4 = train_Y.reset_index(drop=True).append(t)\n",
    "train_X4 = train_X.reset_index(drop=True)\n",
    "test_X4 = test_X\n",
    "\n",
    "all_X4 = train_X4.append(test_X4).drop(columns=['real_drug'])\n",
    "\n",
    "features_g = [col for col in train_X4.columns if 'g-' in col]\n",
    "features_c = [col for col in train_X4.columns if 'c-' in col]\n",
    "\n",
    "all_X4['g_sum'] = all_X4[features_g].sum(axis = 1)\n",
    "all_X4['g_mean'] = all_X4[features_g].mean(axis = 1)\n",
    "all_X4['g_std'] = all_X4[features_g].std(axis = 1)\n",
    "all_X4['g_kurt'] = all_X4[features_g].kurtosis(axis = 1)\n",
    "all_X4['g_skew'] = all_X4[features_g].skew(axis = 1)\n",
    "all_X4['c_sum'] = all_X4[features_c].sum(axis = 1)\n",
    "all_X4['c_mean'] = all_X4[features_c].mean(axis = 1)\n",
    "all_X4['c_std'] = all_X4[features_c].std(axis = 1)\n",
    "all_X4['c_kurt'] = all_X4[features_c].kurtosis(axis = 1)\n",
    "all_X4['c_skew'] = all_X4[features_c].skew(axis = 1)\n",
    "all_X4['gc_sum'] = all_X4[features_g + features_c].sum(axis = 1)\n",
    "all_X4['gc_mean'] = all_X4[features_g + features_c].mean(axis = 1)\n",
    "all_X4['gc_std'] = all_X4[features_g + features_c].std(axis = 1)\n",
    "all_X4['gc_kurt'] = all_X4[features_g + features_c].kurtosis(axis = 1)\n",
    "all_X4['gc_skew'] = all_X4[features_g + features_c].skew(axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GaussRankScaler()\n",
    "all_X4 = scaler.fit_transform(all_X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformer = PCA(687)\n",
    "all_X4 = pca_transformer.fit_transform(all_X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X4 = all_X4[:train_X4.shape[0]]\n",
    "test_X4 = all_X4[train_X4.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X4 = np.vstack([train_X4, test_X4[test_X['real_drug'] == False]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_smoothing = 0.001\n",
    "train_Y4 = (1 - alpha_smoothing) * train_Y4 + alpha_smoothing * train_Y4.mean(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y4 = train_Y4.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fold 1\n",
      "Epoch 1/50   -   loss: 0.36709   -   val_loss: 0.06566\n",
      "Epoch 2/50   -   loss: 0.04150   -   val_loss: 0.02564\n",
      "Epoch 3/50   -   loss: 0.02403   -   val_loss: 0.02049\n",
      "Epoch 4/50   -   loss: 0.02037   -   val_loss: 0.01863\n",
      "Epoch 5/50   -   loss: 0.01872   -   val_loss: 0.01771\n",
      "Epoch 6/50   -   loss: 0.01765   -   val_loss: 0.01711\n",
      "Epoch 7/50   -   loss: 0.01686   -   val_loss: 0.01664\n",
      "Epoch 8/50   -   loss: 0.01628   -   val_loss: 0.01641\n",
      "Epoch 9/50   -   loss: 0.01581   -   val_loss: 0.01620\n",
      "Epoch 10/50   -   loss: 0.01549   -   val_loss: 0.01605\n",
      "Epoch 11/50   -   loss: 0.01503   -   val_loss: 0.01593\n",
      "Epoch 12/50   -   loss: 0.01455   -   val_loss: 0.01573\n",
      "Epoch 13/50   -   loss: 0.01419   -   val_loss: 0.01565\n",
      "Epoch 14/50   -   loss: 0.01389   -   val_loss: 0.01559\n",
      "Epoch 15/50   -   loss: 0.01348   -   val_loss: 0.01552\n",
      "Epoch 16/50   -   loss: 0.01312   -   val_loss: 0.01549\n",
      "Epoch 17/50   -   loss: 0.01272   -   val_loss: 0.01551\n",
      "Epoch 18/50   -   loss: 0.01244   -   val_loss: 0.01557\n",
      "Epoch 19/50   -   loss: 0.01210   -   val_loss: 0.01554\n",
      "Epoch 20/50   -   loss: 0.01171   -   val_loss: 0.01546\n",
      "Epoch 21/50   -   loss: 0.01134   -   val_loss: 0.01548\n",
      "Epoch 22/50   -   loss: 0.01105   -   val_loss: 0.01553\n",
      "Epoch 23/50   -   loss: 0.01073   -   val_loss: 0.01554\n",
      "Epoch 24/50   -   loss: 0.01045   -   val_loss: 0.01567\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 25/50   -   loss: 0.00918   -   val_loss: 0.01558\n",
      "Epoch 26/50   -   loss: 0.00887   -   val_loss: 0.01556\n",
      "Epoch 27/50   -   loss: 0.00863   -   val_loss: 0.01554\n",
      "Epoch 28/50   -   loss: 0.00844   -   val_loss: 0.01555\n",
      "Epoch 29/50   -   loss: 0.00819   -   val_loss: 0.01560\n",
      "Epoch 30/50   -   loss: 0.00796   -   val_loss: 0.01563\n",
      "Epoch 31/50   -   loss: 0.00776   -   val_loss: 0.01562\n",
      "Epoch 32/50   -   loss: 0.00753   -   val_loss: 0.01564\n",
      "Epoch 33/50   -   loss: 0.00732   -   val_loss: 0.01567\n",
      "Epoch 34/50   -   loss: 0.00710   -   val_loss: 0.01574\n",
      "Epoch 35/50   -   loss: 0.00688   -   val_loss: 0.01580\n",
      "Epoch 36/50   -   loss: 0.00667   -   val_loss: 0.01584\n",
      "Epoch 37/50   -   loss: 0.00653   -   val_loss: 0.01595\n",
      "Epoch 38/50   -   loss: 0.00631   -   val_loss: 0.01596\n",
      "Epoch 39/50   -   loss: 0.00608   -   val_loss: 0.01599\n",
      "Epoch 40/50   -   loss: 0.00594   -   val_loss: 0.01605\n",
      "Epoch 41/50   -   loss: 0.00571   -   val_loss: 0.01612\n",
      "Epoch 42/50   -   loss: 0.00558   -   val_loss: 0.01610\n",
      "Epoch 43/50   -   loss: 0.00539   -   val_loss: 0.01621\n",
      "Epoch 44/50   -   loss: 0.00521   -   val_loss: 0.01628\n",
      "Epoch 45/50   -   loss: 0.00498   -   val_loss: 0.01632\n",
      "Epoch 46/50   -   loss: 0.00487   -   val_loss: 0.01640\n",
      "Epoch 47/50   -   loss: 0.00472   -   val_loss: 0.01647\n",
      "Epoch 48/50   -   loss: 0.00462   -   val_loss: 0.01651\n",
      "Epoch 49/50   -   loss: 0.00447   -   val_loss: 0.01663\n",
      "Epoch 50/50   -   loss: 0.00433   -   val_loss: 0.01669\n",
      "Train fold 2\n",
      "Epoch 1/50   -   loss: 0.36760   -   val_loss: 0.06741\n",
      "Epoch 2/50   -   loss: 0.04078   -   val_loss: 0.02493\n",
      "Epoch 3/50   -   loss: 0.02393   -   val_loss: 0.02058\n",
      "Epoch 4/50   -   loss: 0.02045   -   val_loss: 0.01875\n",
      "Epoch 5/50   -   loss: 0.01877   -   val_loss: 0.01788\n",
      "Epoch 6/50   -   loss: 0.01765   -   val_loss: 0.01731\n",
      "Epoch 7/50   -   loss: 0.01694   -   val_loss: 0.01697\n",
      "Epoch 8/50   -   loss: 0.01632   -   val_loss: 0.01669\n",
      "Epoch 9/50   -   loss: 0.01579   -   val_loss: 0.01643\n",
      "Epoch 10/50   -   loss: 0.01537   -   val_loss: 0.01624\n",
      "Epoch 11/50   -   loss: 0.01493   -   val_loss: 0.01609\n",
      "Epoch 12/50   -   loss: 0.01457   -   val_loss: 0.01599\n",
      "Epoch 13/50   -   loss: 0.01416   -   val_loss: 0.01592\n",
      "Epoch 14/50   -   loss: 0.01380   -   val_loss: 0.01575\n",
      "Epoch 15/50   -   loss: 0.01343   -   val_loss: 0.01568\n",
      "Epoch 16/50   -   loss: 0.01308   -   val_loss: 0.01561\n",
      "Epoch 17/50   -   loss: 0.01274   -   val_loss: 0.01564\n",
      "Epoch 18/50   -   loss: 0.01238   -   val_loss: 0.01571\n",
      "Epoch 19/50   -   loss: 0.01200   -   val_loss: 0.01574\n",
      "Epoch 20/50   -   loss: 0.01176   -   val_loss: 0.01572\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 21/50   -   loss: 0.01076   -   val_loss: 0.01562\n",
      "Epoch 22/50   -   loss: 0.01051   -   val_loss: 0.01555\n",
      "Epoch 23/50   -   loss: 0.01031   -   val_loss: 0.01555\n",
      "Epoch 24/50   -   loss: 0.01017   -   val_loss: 0.01556\n",
      "Epoch 25/50   -   loss: 0.01004   -   val_loss: 0.01557\n",
      "Epoch 26/50   -   loss: 0.00985   -   val_loss: 0.01558\n",
      "Epoch 27/50   -   loss: 0.00972   -   val_loss: 0.01558\n",
      "Epoch 28/50   -   loss: 0.00960   -   val_loss: 0.01556\n",
      "Epoch 29/50   -   loss: 0.00938   -   val_loss: 0.01563\n",
      "Epoch 30/50   -   loss: 0.00928   -   val_loss: 0.01564\n",
      "Epoch 31/50   -   loss: 0.00910   -   val_loss: 0.01566\n",
      "Epoch 32/50   -   loss: 0.00892   -   val_loss: 0.01569\n",
      "Epoch 33/50   -   loss: 0.00872   -   val_loss: 0.01568\n",
      "Epoch 34/50   -   loss: 0.00859   -   val_loss: 0.01570\n",
      "Epoch 35/50   -   loss: 0.00838   -   val_loss: 0.01575\n",
      "Epoch 36/50   -   loss: 0.00818   -   val_loss: 0.01574\n",
      "Epoch 37/50   -   loss: 0.00799   -   val_loss: 0.01585\n",
      "Epoch 38/50   -   loss: 0.00778   -   val_loss: 0.01583\n",
      "Epoch 39/50   -   loss: 0.00756   -   val_loss: 0.01591\n",
      "Epoch 40/50   -   loss: 0.00737   -   val_loss: 0.01596\n",
      "Epoch 41/50   -   loss: 0.00712   -   val_loss: 0.01599\n",
      "Epoch 42/50   -   loss: 0.00692   -   val_loss: 0.01603\n",
      "Epoch 43/50   -   loss: 0.00671   -   val_loss: 0.01611\n",
      "Epoch 44/50   -   loss: 0.00648   -   val_loss: 0.01619\n",
      "Epoch 45/50   -   loss: 0.00629   -   val_loss: 0.01623\n",
      "Epoch 46/50   -   loss: 0.00613   -   val_loss: 0.01631\n",
      "Epoch 47/50   -   loss: 0.00589   -   val_loss: 0.01634\n",
      "Epoch 48/50   -   loss: 0.00575   -   val_loss: 0.01641\n",
      "Epoch 49/50   -   loss: 0.00551   -   val_loss: 0.01649\n",
      "Epoch 50/50   -   loss: 0.00530   -   val_loss: 0.01650\n",
      "Train fold 3\n",
      "Epoch 1/50   -   loss: 0.36866   -   val_loss: 0.06169\n",
      "Epoch 2/50   -   loss: 0.04100   -   val_loss: 0.02384\n",
      "Epoch 3/50   -   loss: 0.02401   -   val_loss: 0.01961\n",
      "Epoch 4/50   -   loss: 0.02041   -   val_loss: 0.01815\n",
      "Epoch 5/50   -   loss: 0.01889   -   val_loss: 0.01737\n",
      "Epoch 6/50   -   loss: 0.01777   -   val_loss: 0.01681\n",
      "Epoch 7/50   -   loss: 0.01708   -   val_loss: 0.01652\n",
      "Epoch 8/50   -   loss: 0.01636   -   val_loss: 0.01624\n",
      "Epoch 9/50   -   loss: 0.01591   -   val_loss: 0.01603\n",
      "Epoch 10/50   -   loss: 0.01547   -   val_loss: 0.01582\n",
      "Epoch 11/50   -   loss: 0.01504   -   val_loss: 0.01573\n",
      "Epoch 12/50   -   loss: 0.01466   -   val_loss: 0.01557\n",
      "Epoch 13/50   -   loss: 0.01431   -   val_loss: 0.01548\n",
      "Epoch 14/50   -   loss: 0.01389   -   val_loss: 0.01544\n",
      "Epoch 15/50   -   loss: 0.01353   -   val_loss: 0.01531\n",
      "Epoch 16/50   -   loss: 0.01316   -   val_loss: 0.01533\n",
      "Epoch 17/50   -   loss: 0.01275   -   val_loss: 0.01531\n",
      "Epoch 18/50   -   loss: 0.01252   -   val_loss: 0.01530\n",
      "Epoch 19/50   -   loss: 0.01210   -   val_loss: 0.01528\n",
      "Epoch 20/50   -   loss: 0.01175   -   val_loss: 0.01529\n",
      "Epoch 21/50   -   loss: 0.01142   -   val_loss: 0.01529\n",
      "Epoch 22/50   -   loss: 0.01115   -   val_loss: 0.01540\n",
      "Epoch 23/50   -   loss: 0.01082   -   val_loss: 0.01543\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 24/50   -   loss: 0.00960   -   val_loss: 0.01533\n",
      "Epoch 25/50   -   loss: 0.00932   -   val_loss: 0.01536\n",
      "Epoch 26/50   -   loss: 0.00908   -   val_loss: 0.01534\n",
      "Epoch 27/50   -   loss: 0.00891   -   val_loss: 0.01538\n",
      "Epoch 28/50   -   loss: 0.00871   -   val_loss: 0.01537\n",
      "Epoch 29/50   -   loss: 0.00850   -   val_loss: 0.01540\n",
      "Epoch 30/50   -   loss: 0.00825   -   val_loss: 0.01541\n",
      "Epoch 31/50   -   loss: 0.00807   -   val_loss: 0.01547\n",
      "Epoch 32/50   -   loss: 0.00790   -   val_loss: 0.01549\n",
      "Epoch 33/50   -   loss: 0.00767   -   val_loss: 0.01555\n",
      "Epoch 34/50   -   loss: 0.00743   -   val_loss: 0.01556\n",
      "Epoch 35/50   -   loss: 0.00729   -   val_loss: 0.01559\n",
      "Epoch 36/50   -   loss: 0.00710   -   val_loss: 0.01564\n",
      "Epoch 37/50   -   loss: 0.00686   -   val_loss: 0.01570\n",
      "Epoch 38/50   -   loss: 0.00669   -   val_loss: 0.01575\n",
      "Epoch 39/50   -   loss: 0.00642   -   val_loss: 0.01578\n",
      "Epoch 40/50   -   loss: 0.00620   -   val_loss: 0.01591\n",
      "Epoch 41/50   -   loss: 0.00606   -   val_loss: 0.01597\n",
      "Epoch 42/50   -   loss: 0.00590   -   val_loss: 0.01597\n",
      "Epoch 43/50   -   loss: 0.00569   -   val_loss: 0.01602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50   -   loss: 0.00549   -   val_loss: 0.01609\n",
      "Epoch 45/50   -   loss: 0.00533   -   val_loss: 0.01620\n",
      "Epoch 46/50   -   loss: 0.00516   -   val_loss: 0.01626\n",
      "Epoch 47/50   -   loss: 0.00501   -   val_loss: 0.01629\n",
      "Epoch 48/50   -   loss: 0.00490   -   val_loss: 0.01639\n",
      "Epoch 49/50   -   loss: 0.00470   -   val_loss: 0.01646\n",
      "Epoch 50/50   -   loss: 0.00454   -   val_loss: 0.01649\n",
      "Train fold 4\n",
      "Epoch 1/50   -   loss: 0.36530   -   val_loss: 0.05661\n",
      "Epoch 2/50   -   loss: 0.04129   -   val_loss: 0.02345\n",
      "Epoch 3/50   -   loss: 0.02445   -   val_loss: 0.01969\n",
      "Epoch 4/50   -   loss: 0.02038   -   val_loss: 0.01837\n",
      "Epoch 5/50   -   loss: 0.01876   -   val_loss: 0.01758\n",
      "Epoch 6/50   -   loss: 0.01763   -   val_loss: 0.01716\n",
      "Epoch 7/50   -   loss: 0.01692   -   val_loss: 0.01680\n",
      "Epoch 8/50   -   loss: 0.01636   -   val_loss: 0.01650\n",
      "Epoch 9/50   -   loss: 0.01584   -   val_loss: 0.01629\n",
      "Epoch 10/50   -   loss: 0.01544   -   val_loss: 0.01617\n",
      "Epoch 11/50   -   loss: 0.01503   -   val_loss: 0.01597\n",
      "Epoch 12/50   -   loss: 0.01467   -   val_loss: 0.01582\n",
      "Epoch 13/50   -   loss: 0.01426   -   val_loss: 0.01570\n",
      "Epoch 14/50   -   loss: 0.01387   -   val_loss: 0.01566\n",
      "Epoch 15/50   -   loss: 0.01347   -   val_loss: 0.01566\n",
      "Epoch 16/50   -   loss: 0.01313   -   val_loss: 0.01560\n",
      "Epoch 17/50   -   loss: 0.01279   -   val_loss: 0.01559\n",
      "Epoch 18/50   -   loss: 0.01238   -   val_loss: 0.01559\n",
      "Epoch 19/50   -   loss: 0.01206   -   val_loss: 0.01565\n",
      "Epoch 20/50   -   loss: 0.01169   -   val_loss: 0.01557\n",
      "Epoch 21/50   -   loss: 0.01144   -   val_loss: 0.01572\n",
      "Epoch 22/50   -   loss: 0.01106   -   val_loss: 0.01567\n",
      "Epoch 23/50   -   loss: 0.01077   -   val_loss: 0.01567\n",
      "Epoch 24/50   -   loss: 0.01045   -   val_loss: 0.01579\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 25/50   -   loss: 0.00925   -   val_loss: 0.01572\n",
      "Epoch 26/50   -   loss: 0.00895   -   val_loss: 0.01572\n",
      "Epoch 27/50   -   loss: 0.00866   -   val_loss: 0.01573\n",
      "Epoch 28/50   -   loss: 0.00849   -   val_loss: 0.01571\n",
      "Epoch 29/50   -   loss: 0.00825   -   val_loss: 0.01574\n",
      "Epoch 30/50   -   loss: 0.00794   -   val_loss: 0.01579\n",
      "Epoch 31/50   -   loss: 0.00778   -   val_loss: 0.01585\n",
      "Epoch 32/50   -   loss: 0.00757   -   val_loss: 0.01588\n",
      "Epoch 33/50   -   loss: 0.00738   -   val_loss: 0.01595\n",
      "Epoch 34/50   -   loss: 0.00717   -   val_loss: 0.01596\n",
      "Epoch 35/50   -   loss: 0.00696   -   val_loss: 0.01605\n",
      "Epoch 36/50   -   loss: 0.00675   -   val_loss: 0.01606\n",
      "Epoch 37/50   -   loss: 0.00654   -   val_loss: 0.01606\n",
      "Epoch 38/50   -   loss: 0.00637   -   val_loss: 0.01621\n",
      "Epoch 39/50   -   loss: 0.00614   -   val_loss: 0.01625\n",
      "Epoch 40/50   -   loss: 0.00597   -   val_loss: 0.01631\n",
      "Epoch 41/50   -   loss: 0.00576   -   val_loss: 0.01637\n",
      "Epoch 42/50   -   loss: 0.00563   -   val_loss: 0.01644\n",
      "Epoch 43/50   -   loss: 0.00545   -   val_loss: 0.01654\n",
      "Epoch 44/50   -   loss: 0.00529   -   val_loss: 0.01658\n",
      "Epoch 45/50   -   loss: 0.00511   -   val_loss: 0.01667\n",
      "Epoch 46/50   -   loss: 0.00499   -   val_loss: 0.01668\n",
      "Epoch 47/50   -   loss: 0.00481   -   val_loss: 0.01680\n",
      "Epoch 48/50   -   loss: 0.00466   -   val_loss: 0.01680\n",
      "Epoch 49/50   -   loss: 0.00448   -   val_loss: 0.01694\n",
      "Epoch 50/50   -   loss: 0.00438   -   val_loss: 0.01693\n",
      "Train fold 5\n",
      "Epoch 1/50   -   loss: 0.36592   -   val_loss: 0.06179\n",
      "Epoch 2/50   -   loss: 0.04087   -   val_loss: 0.02489\n",
      "Epoch 3/50   -   loss: 0.02408   -   val_loss: 0.02028\n",
      "Epoch 4/50   -   loss: 0.02053   -   val_loss: 0.01855\n",
      "Epoch 5/50   -   loss: 0.01887   -   val_loss: 0.01750\n",
      "Epoch 6/50   -   loss: 0.01773   -   val_loss: 0.01703\n",
      "Epoch 7/50   -   loss: 0.01693   -   val_loss: 0.01645\n",
      "Epoch 8/50   -   loss: 0.01639   -   val_loss: 0.01631\n",
      "Epoch 9/50   -   loss: 0.01585   -   val_loss: 0.01614\n",
      "Epoch 10/50   -   loss: 0.01545   -   val_loss: 0.01595\n",
      "Epoch 11/50   -   loss: 0.01502   -   val_loss: 0.01578\n",
      "Epoch 12/50   -   loss: 0.01462   -   val_loss: 0.01562\n",
      "Epoch 13/50   -   loss: 0.01420   -   val_loss: 0.01549\n",
      "Epoch 14/50   -   loss: 0.01380   -   val_loss: 0.01547\n",
      "Epoch 15/50   -   loss: 0.01343   -   val_loss: 0.01543\n",
      "Epoch 16/50   -   loss: 0.01308   -   val_loss: 0.01538\n",
      "Epoch 17/50   -   loss: 0.01267   -   val_loss: 0.01529\n",
      "Epoch 18/50   -   loss: 0.01237   -   val_loss: 0.01537\n",
      "Epoch 19/50   -   loss: 0.01201   -   val_loss: 0.01534\n",
      "Epoch 20/50   -   loss: 0.01167   -   val_loss: 0.01538\n",
      "Epoch 21/50   -   loss: 0.01133   -   val_loss: 0.01532\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 22/50   -   loss: 0.01032   -   val_loss: 0.01525\n",
      "Epoch 23/50   -   loss: 0.01006   -   val_loss: 0.01527\n",
      "Epoch 24/50   -   loss: 0.00986   -   val_loss: 0.01532\n",
      "Epoch 25/50   -   loss: 0.00967   -   val_loss: 0.01529\n",
      "Epoch 26/50   -   loss: 0.00953   -   val_loss: 0.01531\n",
      "Epoch 27/50   -   loss: 0.00938   -   val_loss: 0.01528\n",
      "Epoch 28/50   -   loss: 0.00922   -   val_loss: 0.01534\n",
      "Epoch 29/50   -   loss: 0.00902   -   val_loss: 0.01534\n",
      "Epoch 30/50   -   loss: 0.00880   -   val_loss: 0.01537\n",
      "Epoch 31/50   -   loss: 0.00863   -   val_loss: 0.01542\n",
      "Epoch 32/50   -   loss: 0.00841   -   val_loss: 0.01550\n",
      "Epoch 33/50   -   loss: 0.00821   -   val_loss: 0.01549\n",
      "Epoch 34/50   -   loss: 0.00804   -   val_loss: 0.01554\n",
      "Epoch 35/50   -   loss: 0.00788   -   val_loss: 0.01551\n",
      "Epoch 36/50   -   loss: 0.00766   -   val_loss: 0.01557\n",
      "Epoch 37/50   -   loss: 0.00744   -   val_loss: 0.01565\n",
      "Epoch 38/50   -   loss: 0.00728   -   val_loss: 0.01566\n",
      "Epoch 39/50   -   loss: 0.00708   -   val_loss: 0.01564\n",
      "Epoch 40/50   -   loss: 0.00683   -   val_loss: 0.01569\n",
      "Epoch 41/50   -   loss: 0.00662   -   val_loss: 0.01581\n",
      "Epoch 42/50   -   loss: 0.00642   -   val_loss: 0.01586\n",
      "Epoch 43/50   -   loss: 0.00625   -   val_loss: 0.01586\n",
      "Epoch 44/50   -   loss: 0.00603   -   val_loss: 0.01595\n",
      "Epoch 45/50   -   loss: 0.00584   -   val_loss: 0.01606\n",
      "Epoch 46/50   -   loss: 0.00565   -   val_loss: 0.01611\n",
      "Epoch 47/50   -   loss: 0.00545   -   val_loss: 0.01615\n",
      "Epoch 48/50   -   loss: 0.00529   -   val_loss: 0.01621\n",
      "Epoch 49/50   -   loss: 0.00511   -   val_loss: 0.01627\n",
      "Epoch 50/50   -   loss: 0.00499   -   val_loss: 0.01637\n",
      "Train fold 6\n",
      "Epoch 1/50   -   loss: 0.36909   -   val_loss: 0.06507\n",
      "Epoch 2/50   -   loss: 0.04048   -   val_loss: 0.02538\n",
      "Epoch 3/50   -   loss: 0.02393   -   val_loss: 0.02063\n",
      "Epoch 4/50   -   loss: 0.02059   -   val_loss: 0.01883\n",
      "Epoch 5/50   -   loss: 0.01872   -   val_loss: 0.01791\n",
      "Epoch 6/50   -   loss: 0.01767   -   val_loss: 0.01721\n",
      "Epoch 7/50   -   loss: 0.01687   -   val_loss: 0.01690\n",
      "Epoch 8/50   -   loss: 0.01636   -   val_loss: 0.01665\n",
      "Epoch 9/50   -   loss: 0.01583   -   val_loss: 0.01650\n",
      "Epoch 10/50   -   loss: 0.01539   -   val_loss: 0.01624\n",
      "Epoch 11/50   -   loss: 0.01495   -   val_loss: 0.01603\n",
      "Epoch 12/50   -   loss: 0.01458   -   val_loss: 0.01593\n",
      "Epoch 13/50   -   loss: 0.01418   -   val_loss: 0.01581\n",
      "Epoch 14/50   -   loss: 0.01384   -   val_loss: 0.01572\n",
      "Epoch 15/50   -   loss: 0.01343   -   val_loss: 0.01565\n",
      "Epoch 16/50   -   loss: 0.01308   -   val_loss: 0.01558\n",
      "Epoch 17/50   -   loss: 0.01266   -   val_loss: 0.01554\n",
      "Epoch 18/50   -   loss: 0.01239   -   val_loss: 0.01553\n",
      "Epoch 19/50   -   loss: 0.01196   -   val_loss: 0.01552\n",
      "Epoch 20/50   -   loss: 0.01169   -   val_loss: 0.01562\n",
      "Epoch 21/50   -   loss: 0.01135   -   val_loss: 0.01565\n",
      "Epoch 22/50   -   loss: 0.01104   -   val_loss: 0.01569\n",
      "Epoch 23/50   -   loss: 0.01068   -   val_loss: 0.01569\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 24/50   -   loss: 0.00952   -   val_loss: 0.01565\n",
      "Epoch 25/50   -   loss: 0.00922   -   val_loss: 0.01564\n",
      "Epoch 26/50   -   loss: 0.00899   -   val_loss: 0.01565\n",
      "Epoch 27/50   -   loss: 0.00875   -   val_loss: 0.01566\n",
      "Epoch 28/50   -   loss: 0.00859   -   val_loss: 0.01570\n",
      "Epoch 29/50   -   loss: 0.00836   -   val_loss: 0.01568\n",
      "Epoch 30/50   -   loss: 0.00815   -   val_loss: 0.01574\n",
      "Epoch 31/50   -   loss: 0.00797   -   val_loss: 0.01575\n",
      "Epoch 32/50   -   loss: 0.00773   -   val_loss: 0.01577\n",
      "Epoch 33/50   -   loss: 0.00751   -   val_loss: 0.01582\n",
      "Epoch 34/50   -   loss: 0.00727   -   val_loss: 0.01587\n",
      "Epoch 35/50   -   loss: 0.00711   -   val_loss: 0.01587\n",
      "Epoch 36/50   -   loss: 0.00693   -   val_loss: 0.01593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50   -   loss: 0.00671   -   val_loss: 0.01598\n",
      "Epoch 38/50   -   loss: 0.00643   -   val_loss: 0.01607\n",
      "Epoch 39/50   -   loss: 0.00631   -   val_loss: 0.01608\n",
      "Epoch 40/50   -   loss: 0.00609   -   val_loss: 0.01615\n",
      "Epoch 41/50   -   loss: 0.00592   -   val_loss: 0.01620\n",
      "Epoch 42/50   -   loss: 0.00570   -   val_loss: 0.01627\n",
      "Epoch 43/50   -   loss: 0.00552   -   val_loss: 0.01636\n",
      "Epoch 44/50   -   loss: 0.00535   -   val_loss: 0.01637\n",
      "Epoch 45/50   -   loss: 0.00519   -   val_loss: 0.01639\n",
      "Epoch 46/50   -   loss: 0.00500   -   val_loss: 0.01652\n",
      "Epoch 47/50   -   loss: 0.00490   -   val_loss: 0.01660\n",
      "Epoch 48/50   -   loss: 0.00472   -   val_loss: 0.01663\n",
      "Epoch 49/50   -   loss: 0.00453   -   val_loss: 0.01675\n",
      "Epoch 50/50   -   loss: 0.00448   -   val_loss: 0.01676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 0.011962243980621989, 'val': 0.015516783576458693}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_CV_for_model(Model4, 4, train_X4, train_Y4, test_X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall score is 0.00994\n"
     ]
    }
   ],
   "source": [
    "run_eval_for_model(Model4, 4, train_X4[:train_X.shape[0]][train_X['real_drug']], train_Y4[:train_Y.shape[0]][train_X['real_drug']], test_X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_P4 = run_predict_for_model(Model4, 4, train_X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24172, 206), (24172, 206))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_P4.shape, train_Y4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008982095616991351"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(train_Y4, train_P4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_P4[:train_X.shape[0]][train_X['real_drug'] == False] = 0\n",
    "train_P4[train_X.shape[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008836717233532172"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(train_Y4, train_P4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_P = run_predict_for_model(Model4, 4, test_X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y[list(test_Y.columns)] = test_P\n",
    "test_Y[test_X['real_drug'] == False] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sig_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id_0004d9e33</th>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>0.026022</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>0.001944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_001897cda</th>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.006331</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.006086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.002813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_002429b5b</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_00276f245</th>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.023269</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.007372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>0.027139</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.001656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_0027f1083</th>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.001627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_ff7004b87</th>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.017332</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.038757</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.002238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_ff925dd0d</th>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.039036</td>\n",
       "      <td>0.005091</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>0.004809</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.001588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_ffb710450</th>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.054895</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.015742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.001623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_ffbb869f2</th>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.015097</td>\n",
       "      <td>0.005949</td>\n",
       "      <td>0.003971</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.030438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.002505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_ffd5800b6</th>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.016661</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.001613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3982 rows  206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "sig_id                                                              \n",
       "id_0004d9e33                     0.002171                0.001328   \n",
       "id_001897cda                     0.000631                0.000748   \n",
       "id_002429b5b                     0.000000                0.000000   \n",
       "id_00276f245                     0.000620                0.000638   \n",
       "id_0027f1083                     0.001551                0.001067   \n",
       "...                                   ...                     ...   \n",
       "id_ff7004b87                     0.000926                0.000516   \n",
       "id_ff925dd0d                     0.001859                0.000952   \n",
       "id_ffb710450                     0.001665                0.000586   \n",
       "id_ffbb869f2                     0.000973                0.001788   \n",
       "id_ffd5800b6                     0.001528                0.000651   \n",
       "\n",
       "              acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "sig_id                                                         \n",
       "id_0004d9e33        0.002253                        0.017687   \n",
       "id_001897cda        0.001614                        0.003659   \n",
       "id_002429b5b        0.000000                        0.000000   \n",
       "id_00276f245        0.001117                        0.023269   \n",
       "id_0027f1083        0.001734                        0.013590   \n",
       "...                      ...                             ...   \n",
       "id_ff7004b87        0.001241                        0.002234   \n",
       "id_ff925dd0d        0.001260                        0.007831   \n",
       "id_ffb710450        0.001574                        0.020110   \n",
       "id_ffbb869f2        0.000975                        0.015097   \n",
       "id_ffd5800b6        0.001051                        0.018593   \n",
       "\n",
       "              acetylcholine_receptor_antagonist  \\\n",
       "sig_id                                            \n",
       "id_0004d9e33                           0.026022   \n",
       "id_001897cda                           0.003010   \n",
       "id_002429b5b                           0.000000   \n",
       "id_00276f245                           0.007211   \n",
       "id_0027f1083                           0.015821   \n",
       "...                                         ...   \n",
       "id_ff7004b87                           0.017332   \n",
       "id_ff925dd0d                           0.039036   \n",
       "id_ffb710450                           0.054895   \n",
       "id_ffbb869f2                           0.005949   \n",
       "id_ffd5800b6                           0.016661   \n",
       "\n",
       "              acetylcholinesterase_inhibitor  adenosine_receptor_agonist  \\\n",
       "sig_id                                                                     \n",
       "id_0004d9e33                        0.003741                    0.001546   \n",
       "id_001897cda                        0.002421                    0.002964   \n",
       "id_002429b5b                        0.000000                    0.000000   \n",
       "id_00276f245                        0.006435                    0.001940   \n",
       "id_0027f1083                        0.002559                    0.004305   \n",
       "...                                      ...                         ...   \n",
       "id_ff7004b87                        0.002486                    0.002286   \n",
       "id_ff925dd0d                        0.005091                    0.002150   \n",
       "id_ffb710450                        0.002837                    0.002792   \n",
       "id_ffbb869f2                        0.003971                    0.002500   \n",
       "id_ffd5800b6                        0.004477                    0.002087   \n",
       "\n",
       "              adenosine_receptor_antagonist  adenylyl_cyclase_activator  \\\n",
       "sig_id                                                                    \n",
       "id_0004d9e33                       0.010823                    0.000312   \n",
       "id_001897cda                       0.006331                    0.003306   \n",
       "id_002429b5b                       0.000000                    0.000000   \n",
       "id_00276f245                       0.003666                    0.000217   \n",
       "id_0027f1083                       0.002512                    0.000386   \n",
       "...                                     ...                         ...   \n",
       "id_ff7004b87                       0.002138                    0.000636   \n",
       "id_ff925dd0d                       0.004809                    0.000385   \n",
       "id_ffb710450                       0.003713                    0.000273   \n",
       "id_ffbb869f2                       0.001956                    0.000744   \n",
       "id_ffd5800b6                       0.008500                    0.000269   \n",
       "\n",
       "              adrenergic_receptor_agonist  ...  \\\n",
       "sig_id                                     ...   \n",
       "id_0004d9e33                     0.014732  ...   \n",
       "id_001897cda                     0.006086  ...   \n",
       "id_002429b5b                     0.000000  ...   \n",
       "id_00276f245                     0.007372  ...   \n",
       "id_0027f1083                     0.009790  ...   \n",
       "...                                   ...  ...   \n",
       "id_ff7004b87                     0.005145  ...   \n",
       "id_ff925dd0d                     0.011100  ...   \n",
       "id_ffb710450                     0.015742  ...   \n",
       "id_ffbb869f2                     0.030438  ...   \n",
       "id_ffd5800b6                     0.003340  ...   \n",
       "\n",
       "              tropomyosin_receptor_kinase_inhibitor  trpv_agonist  \\\n",
       "sig_id                                                              \n",
       "id_0004d9e33                               0.000730      0.003549   \n",
       "id_001897cda                               0.000918      0.000760   \n",
       "id_002429b5b                               0.000000      0.000000   \n",
       "id_00276f245                               0.000355      0.000682   \n",
       "id_0027f1083                               0.000523      0.000220   \n",
       "...                                             ...           ...   \n",
       "id_ff7004b87                               0.000469      0.000968   \n",
       "id_ff925dd0d                               0.000375      0.001357   \n",
       "id_ffb710450                               0.000340      0.001018   \n",
       "id_ffbb869f2                               0.000314      0.000235   \n",
       "id_ffd5800b6                               0.000406      0.000628   \n",
       "\n",
       "              trpv_antagonist  tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "sig_id                                                                        \n",
       "id_0004d9e33         0.003592           0.001130                   0.000549   \n",
       "id_001897cda         0.004370           0.000536                   0.003929   \n",
       "id_002429b5b         0.000000           0.000000                   0.000000   \n",
       "id_00276f245         0.004195           0.027139                   0.004764   \n",
       "id_0027f1083         0.002824           0.001352                   0.000399   \n",
       "...                       ...                ...                        ...   \n",
       "id_ff7004b87         0.001773           0.038757                   0.008308   \n",
       "id_ff925dd0d         0.001794           0.001146                   0.001845   \n",
       "id_ffb710450         0.002779           0.001120                   0.001397   \n",
       "id_ffbb869f2         0.003044           0.000503                   0.001767   \n",
       "id_ffd5800b6         0.001896           0.001824                   0.001193   \n",
       "\n",
       "              ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  \\\n",
       "sig_id                                                                 \n",
       "id_0004d9e33                               0.000762         0.003203   \n",
       "id_001897cda                               0.000454         0.001155   \n",
       "id_002429b5b                               0.000000         0.000000   \n",
       "id_00276f245                               0.000536         0.001166   \n",
       "id_0027f1083                               0.000425         0.001223   \n",
       "...                                             ...              ...   \n",
       "id_ff7004b87                               0.000429         0.004527   \n",
       "id_ff925dd0d                               0.000708         0.001228   \n",
       "id_ffb710450                               0.000354         0.001211   \n",
       "id_ffbb869f2                               0.000501         0.002656   \n",
       "id_ffd5800b6                               0.000464         0.001816   \n",
       "\n",
       "              vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "sig_id                                                              \n",
       "id_0004d9e33   0.003871                    0.004933       0.001944  \n",
       "id_001897cda   0.000828                    0.001719       0.002813  \n",
       "id_002429b5b   0.000000                    0.000000       0.000000  \n",
       "id_00276f245   0.001717                    0.000462       0.001656  \n",
       "id_0027f1083   0.001345                    0.000292       0.001627  \n",
       "...                 ...                         ...            ...  \n",
       "id_ff7004b87   0.002440                    0.000987       0.002238  \n",
       "id_ff925dd0d   0.001549                    0.000773       0.001588  \n",
       "id_ffb710450   0.001487                    0.000394       0.001623  \n",
       "id_ffbb869f2   0.001747                    0.000293       0.002505  \n",
       "id_ffd5800b6   0.002169                    0.000727       0.001613  \n",
       "\n",
       "[3982 rows x 206 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "papermill": {
   "duration": 469.854705,
   "end_time": "2020-10-22T16:37:38.321100",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-22T16:29:48.466395",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
